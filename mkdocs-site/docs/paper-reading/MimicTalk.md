# MimicTalk解读

ZJU 赵洲老师组的文章

原文链接：https://arxiv.org/html/2410.06734v2

代码：https://github.com/yerfor/MimicTalk

## Background

会说话的人脸生成（TFG）旨在将目标身份的人脸制作成动画，从而创建逼真的会说话视频。个性化 TFG 是一种变体，它强调合成结果的感知身份相似性（从外观和说话风格的角度）。

## Motivation

以往有两种类型的做法，都没法**同时满足普适性和个性化**：

- **依赖身份的方法**针对每个目标人物的视频从头开始训练一个单独的模型。通过**过度拟合一个目标身份**来学习模型中的个性化运动-图像映射。在训练过程中，针对特定人的模型可以隐式地记住目标人物视频中细微的个性化细节，可以训练小规模的依赖于人的模型，获得较高的身份相似性和表达效果。但是**普适性弱且采样效率低**。
- **与身份无关**的 TFG 方法/称单次 TFG 方法在训练过程中结合了各种身份的视频，推理过程中只用一个源图像。然而由于单一输入图像的信息有限，单次方法无法利用目标身份的丰富样本来模仿其**个性化属性**。

## Overview

为此，我们提出了 MimicTalk，首次尝试利用基于 NeRF 的个人识别通用模型中的丰富知识来提高个性化 TFG 的效率和鲁棒性。具体来说，

(1) 我们首先提出了一个与人无关的三维 TFG 模型作为基础模型，并建议将其适配到特定身份中；

(2) 我们提出了一个静态-动态-混合适配管道，以帮助模型学习个性化的静态外观和面部动态特征；

(3) 为了生成个性化说话风格的面部动作，我们提出了一个上下文风格化音频-动作模型，该模型可模仿参考视频中提供的隐式说话风格，而不会因为显式风格表示而造成信息丢失。

## Related Work

### Talking Face Generation/TFG

和motivation里面一样，主要分为依赖身份的方法和身份无关的方法：

- 与身份无关的方法主要针对单次拍摄场景：只向模型提供一张图像，并将其制作成动画以创建视频。因此，这些方法使用**大量的身份视频数据训练大规模通用模型**，以便在推理阶段将其**泛化**到未见过的照片上。
- 另一方面，与身份相关的方法侧重于为**特定**说话者实现更好的视频质量：通常使用**目标用户的一段视频**作为训练数据，并期望模型模拟该说话者的个性化特征，即所谓的个性化 TFG。

### Expressive Facial Motion Generation

大多数方法都依赖于**中间体**来表示说话的风格：

- 外部生成模型对音频-运动映射进行显式建模
- 从任意运动序列中提取的风格向量以实现明确的说话风格控制
- 将表情风格和嘴唇动作分离开，构建一个记忆库以生成表情生动的唇语同步视频
- 将之前的音频/动作latent添加到潜在扩散模型的输入中，以保持时间上的一致性

## Method

### NeRF-based Person-Agnostic Renderer

- 为了保证普适性和效率，考虑利用与**身份无关的TFG方法已经训练好的模型当先验信息**

- 总体参考Real3D-Portrait的基础，这一块感觉是纯粹的combination，没有自己去训练，得到一个**预训练的人脸识别渲染器**
- 具体步骤：
  1. 从一个给定的人脸图像$I_{src}$，在一个基于SegFormer的模型上，重建得到一个利用**三平面表示法**表示的三维人脸模型$P_{cano}$
  
  2. 对于三维人脸模型$P_{cano}$，利用基于SegFormer的运动适配器（motion adapter），结合投影坐标归一化代码$PNCC_{src},PNCC_{tgt}$，来控制面部表情；加入面部表情的三维结果（对应着下文的target image）会被一个基于Mip-NeRF的体渲染器（volume renderer）在特定的摄像机视角$cam_{tgt}$下进行渲染，得到一个低分辨率的图像$I_{raw}$。具体公式为：
     $$
     I_{raw}=\text{VolumeRenderer}⁢(P_{cano}+\text{MotionAdapter}⁢([PNCC_{src},PNCC_{tgt}]),cam_{tgt})
     $$
  
  3. 对上一步得到的低分辨率的图像$I_{raw}$，用超分辨率模块处理一下，得到高分辨率的预测图像：
     $$
     I_{pred}=\text{SuperResolution}(I_{raw})
     $$
- 这个模块的作用就是：在不做任何调整的情况下合成未见身份的会说话的人脸视频

### Static-Dynamic-Hybrid Identity Adaptation

只利用上个模块的方法合成的视频是不够的，因为上个模块的方法与之前的人像无关方法之间存在着明显的**身份相似性差距**：

1. 静态相似性（static similarity），即测量生成的帧是否与目标身份具有相同的纹理（如皱纹、牙齿和头发）或几何细节
2. 动态相似性（dynamic similarity），即描述输入运动条件与输出面部图像中面部肌肉/躯干运动之间的关系

>  这个很好理解，在大量人的说话视频上训练出来的模型，只会趋向于学习到这些人的共性，但是对于个性化的部分难以准确表达。

因此，本文提出了一种高效的静态-动态-混合（SD-Hybrid）适应管道，以实现良好的静态/动态身份相似性，具体来说分为：

- Tri-Plane Inversion处理静态相似性：由于之前提取的$P_{cano}$是从**单个image生成**的，存在信息偏差/损失，这是静态相似性差的主要原因。对此借鉴了GAN inversion的方法，本文将人脸表征$P_{cano}$视为**可学习参数**$P_{cano}^*$，**并对其进行优化**，以最大化静态身份相似度。用视频的第一帧图像到三维的预测值初始化可学习的三平面，然后与其他参数一起优化三平面。

> 这个思路感觉偏向于“PS”预处理，refine一下三维模型，只是采用inversion的方法去做。

- Injecting LoRAs处理动态相似性：由于人脸识别模型是在多说话者人脸对话数据集中学习运动到图像的**平均映射**，因此我们需要根据目标人物的视频**调整通用模型**，以学习其**个性化**的面部动态。为此，本文采用LoRA：通过向**每个线性层和卷积核注入低阶可学习矩阵**。LoRA 可以插入到我们的模型中，分为：
  - 对于motion-adapter，插入$\theta_1^*$；对于volume-renderer，插入$\theta_2^*$；对于super-resolution，插入$\theta_3^*$。

> 考虑到目标人物视频的模型容量大、数据规模小，直接微调整个大模型/模型后面几层会有问题，如 GPU 内存占用大、训练不稳定和灾难性遗忘等。LoRA适合于对大模型进行微调，最近用的也比较多。通过只更新LoRA的方式，可以起到很好的效果。

整个过程的formula为：

$$I_{pred}=\text{SuperResolution}(\text{VolumeRenderer}⁢(P_{cano}^*+\text{MotionAdapter}⁢([PNCC_{src},PNCC_{tgt}],\theta_1^*),cam_{tgt},\theta_2^*),\theta_3^*)$$。

在训练过程中，取目标人物的一小段视频切片当做训练数据，去更新$P_{cano}^*$和$\theta_1^*$，$\theta_2^*$，$\theta_3^*$。

损失函数考虑了：L1损失$\mathcal{L}_1$，LPIPS损失$\mathcal{L}_{LPIPS}$，ID损失$\mathcal{L}_{ID}$，最后这三者进行线性组合。

> 损失函数这个设计比较常见。

### In-Context Stylized Audio-to-Motion

以上各节中组成统一的运动条件说话脸部生成框架。这一节的上下文风格化音频-动作（ICS-A2M）模型，为**音频驱动的场景**生成个性化的面部动作，具体包括：

- 音频引导的动作填充任务
  受LLM和文本到语音合成中**上下文学习方法**启发，本文设计音频引导的运动填充任务：音频和动作对在时间上进行对齐，在信道上进行串联，并由音频-动作模型进行处理。

  - **训练**过程中**随机**屏蔽动作轨迹中的几个片段，并利用屏蔽片段上的**运动重建误差**对模型进行训练。由于该模型**可获得**之前/之后段未屏蔽的运动和完整的音轨，因此它**能学会**利用运动上下文中的说话方式来更准确地预测被屏蔽的共同语音运动。

  - **推理**过程中将参考音频-动作对作为说话方式提示，将任意的驱动音频作为条件，将预测动作的噪声占位符作为模型的输入。这样，模型就能根据风格提示中提供的说话风格预测与音频同步的面部动作。

- 音频-动作流匹配模型

  为了更好地**生成expressive facial motion**，本文利用**流匹配**的方式，模型的参数是transformer $\theta$，预测的流场$\phi$储存某个点$x_t$指向目标分布的速度$v_t=\frac{dx_t}{dt}$。时间步$t$在inference的阶段会从0逐渐变到1。

  >  先验数据点$x_0 \sim \mathcal{N}(0,I)$，目标数据点$x_1 \sim q(x)$，$q(x)$是实际的目标分布/ground truth。

  模型的输入是风格提示$s$、音频条件$a$和噪声运动$x_t$。模型的输出是噪声运动$x_t$对应的速度$v_t(x,a,s;\theta)$。利用条件流匹配目标/CFM来训练：$\mathcal{L}_{CFM}=\mathbf{E}_{t,q(x),p(x|x_1)}||u_t(x|x_1)−v_t(x,a,s;\theta)||^2_2$。这个目标旨在最小化ground truth速度$u_t(x|x_1)$（从$x_t$到$x_1$的唯一的最优传输路径）与模型预测结果速度$v_t(x,a,s;\theta)$之间的差距。

  > 流匹配的方法，之前很少见过。

  除了条件流匹配目标外，本文在去噪样本$x_1$上添加**唇音**损失，以生成准确的面部动作。首先求解ODE 问题：

$$
\frac{dx_t}{dt}=u_t\approx v_t(x_t,\theta), s.t. x_0\sim p(x), t\in [0,1]
$$

​		求解得到的速度$v_t(x,a,s;\theta)$获得预测的样本$\hat{x_1}$，把$\hat{x_1}$输入到SyncNet获得判别同步损失$\mathcal{L}_{sync}$。唇音损失是额外对嘴唇进行调整的。最后总的损失就是$\mathcal{L}_{CFM}$与$\mathcal{L}_{sync}$线性组合。

- 增强风格模仿的无分类指导

  无分类器引导/CFG在扩散模型采样过程中操纵条件强度，也助于**提高风格化音频到运动模型风格模仿质量**。本文将有/无风格提示的网络预测混合构建CFG速度$v_t^{CFG}=v_t(x,a,s;\theta)+\omega (v_t(x,a,s;\theta)-v_t(x,a,0;\theta))$

